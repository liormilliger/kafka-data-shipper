DevOps Lead Home Assignment

In this task you will create a data pipeline using MySQL, Kafka and 2 services that are going to move data back and forth between the Kafka and MySQL data sources.
You are going to deploy this whole stack on Kubernetes cluster.

Your tasks are to:
Deploy MySQL & Kafka on a K8s cluster
Choose the best deployment method you are going to implement it - take into account that you are going to run it in multiple environments.
Both services need to be scalable, highly available.
Both services require persistence - in case of failure data is kept being kept.
Select the correct quorum for Kafka.
On the MySQL initial startup the container will import SQL file containing the data that will later on be shipped to Kafka from the following location https://github.com/surielb/devops-test/blob/master/init.sql
On the Kafka initial startup the container will create topics that will later on be used by the consumer\producer services.
Deploy the 2 data shipper services on the same K8s cluster
The services are located in Github:
Producer - https://github.com/surielb/devops-test/tree/master/producer
Consumer - https://github.com/surielb/devops-test/tree/master/consumer
Containerize the services following Docker best practices and the build guidance in the Github repository. 
Use the same deployment method you did in task 1.



Success criteria:
Complete data flow is working:
MySQL SQL data import on startup
Kafka topics are created on startup
Producer queries data from MySQL and pushes it to Kafka
Consumer pulls data from Kafka and inserts it to MySQL
Best practices all the way, you are building a robust data pipeline infrastructure.
Create a private Github repository and push the code there - make sure you use comments and documentation in your code - include all files\commands\scripts\etc you used to accomplish the task.